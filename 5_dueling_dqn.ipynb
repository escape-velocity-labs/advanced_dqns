{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCxxWBZioi0N"
      },
      "source": [
        "# Dueling Deep Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJIVLLT1nYMl"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y xvfb\n",
        "\n",
        "!pip install pygame pytorch-lightning==1.5.8 pyvirtualdisplay\n",
        "\n",
        "!pip install git+https://github.com/GrupoTuring/PyGame-Learning-Environment\n",
        "!pip install git+https://github.com/lusob/gym-ple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOSJl-X7zvs4"
      },
      "source": [
        "#### Setup virtual display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-Z6takfzqGk"
      },
      "outputs": [],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "Display(visible=False, size=(1400, 900)).start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz8DLleGz_TF"
      },
      "source": [
        "#### Import the necessary code libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP5t6U7-nYoc"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "import random\n",
        "import gym\n",
        "import gym_ple\n",
        "import matplotlib\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "\n",
        "from collections import deque, namedtuple\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import IterableDataset\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "\n",
        "from gym.wrappers import TransformObservation\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "num_gpus = torch.cuda.device_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_IrPlU1wwPx"
      },
      "outputs": [],
      "source": [
        "# Copied from: https://colab.research.google.com/github/deepmind/dm_control/blob/master/tutorial.ipynb#scrollTo=gKc1FNhKiVJX\n",
        "\n",
        "def display_video(frames, framerate=30):\n",
        "  height, width, _ = frames[0].shape\n",
        "  dpi = 70\n",
        "  orig_backend = matplotlib.get_backend()\n",
        "  matplotlib.use('Agg')\n",
        "  fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n",
        "  matplotlib.use(orig_backend)\n",
        "  ax.set_axis_off()\n",
        "  ax.set_aspect('equal')\n",
        "  ax.set_position([0, 0, 1, 1])\n",
        "  im = ax.imshow(frames[0])\n",
        "  def update(frame):\n",
        "    im.set_data(frame)\n",
        "    return [im]\n",
        "  interval = 1000/framerate\n",
        "  anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
        "                                  interval=interval, blit=True, repeat=False)\n",
        "  return HTML(anim.to_html5_video())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLH52SgC0RRI"
      },
      "source": [
        "#### Create the Deep Q-Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6gm8-15nYq7"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, obs_size, n_actions):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(obs_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, hidden_size),\n",
        "        nn.ReLU()    \n",
        "    )\n",
        "\n",
        "    self.fc_adv = nn.Linear(hidden_size, n_actions) \n",
        "    self.fc_value = nn.Linear(hidden_size, 1)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.net(x.float())\n",
        "    adv = self.fc_adv(x)\n",
        "    value = self.fc_value(x)\n",
        "    return value + adv - torch.mean(adv, dim=1, keepdim=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnk0wSWj0hAz"
      },
      "source": [
        "#### Create the policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9a0b9cdnYtT"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy(state, env, net, epsilon=0.0):\n",
        "  if np.random.random() < epsilon:\n",
        "    action = env.action_space.sample()\n",
        "  else:\n",
        "    state = torch.tensor([state]).to(device)\n",
        "    q_values = net(state)\n",
        "    _, action = torch.max(q_values, dim=1)\n",
        "    action = int(action.item())\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brJmKGkl0jge"
      },
      "source": [
        "#### Create the replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvHMYqlZnYvj"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "\n",
        "  def __init__(self, capacity):\n",
        "    self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "  \n",
        "  def append(self, experience):\n",
        "    self.buffer.append(experience)\n",
        "  \n",
        "  def sample(self, batch_size):\n",
        "    return random.sample(self.buffer, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUQcRQ4xnYyI"
      },
      "outputs": [],
      "source": [
        "class RLDataset(IterableDataset):\n",
        "\n",
        "  def __init__(self, buffer, sample_size=400):\n",
        "    self.buffer = buffer\n",
        "    self.sample_size = sample_size\n",
        "  \n",
        "  def __iter__(self):\n",
        "    for experience in self.buffer.sample(self.sample_size):\n",
        "      yield experience"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yvDC9qF0oPr"
      },
      "source": [
        "#### Create the environment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RunningMeanStd:\n",
        "    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n",
        "    def __init__(self, epsilon=1e-4, shape=()):\n",
        "        self.mean = np.zeros(shape, \"float64\")\n",
        "        self.var = np.ones(shape, \"float64\")\n",
        "        self.count = epsilon\n",
        "\n",
        "    def update(self, x):\n",
        "        batch_mean = np.mean(x, axis=0)\n",
        "        batch_var = np.var(x, axis=0)\n",
        "        batch_count = x.shape[0]\n",
        "        self.update_from_moments(batch_mean, batch_var, batch_count)\n",
        "\n",
        "    def update_from_moments(self, batch_mean, batch_var, batch_count):\n",
        "        self.mean, self.var, self.count = update_mean_var_count_from_moments(\n",
        "            self.mean, self.var, self.count, batch_mean, batch_var, batch_count\n",
        "        )\n",
        "\n",
        "\n",
        "def update_mean_var_count_from_moments(\n",
        "    mean, var, count, batch_mean, batch_var, batch_count\n",
        "):\n",
        "    delta = batch_mean - mean\n",
        "    tot_count = count + batch_count\n",
        "\n",
        "    new_mean = mean + delta * batch_count / tot_count\n",
        "    m_a = var * count\n",
        "    m_b = batch_var * batch_count\n",
        "    M2 = m_a + m_b + np.square(delta) * count * batch_count / tot_count\n",
        "    new_var = M2 / tot_count\n",
        "    new_count = tot_count\n",
        "\n",
        "    return new_mean, new_var, new_count\n",
        "\n",
        "\n",
        "class NormalizeObservation(gym.core.Wrapper):\n",
        "    def __init__(\n",
        "        self,\n",
        "        env,\n",
        "        epsilon=1e-8,\n",
        "    ):\n",
        "        super().__init__(env)\n",
        "        self.num_envs = getattr(env, \"num_envs\", 1)\n",
        "        self.is_vector_env = getattr(env, \"is_vector_env\", False)\n",
        "        if self.is_vector_env:\n",
        "            self.obs_rms = RunningMeanStd(shape=self.single_observation_space.shape)\n",
        "        else:\n",
        "            self.obs_rms = RunningMeanStd(shape=self.observation_space.shape)\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, rews, dones, infos = self.env.step(action)\n",
        "        if self.is_vector_env:\n",
        "            obs = self.normalize(obs)\n",
        "        else:\n",
        "            obs = self.normalize(np.array([obs]))[0]\n",
        "        return obs, rews, dones, infos\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        return_info = kwargs.get(\"return_info\", False)\n",
        "        if return_info:\n",
        "            obs, info = self.env.reset(**kwargs)\n",
        "        else:\n",
        "            obs = self.env.reset(**kwargs)\n",
        "        if self.is_vector_env:\n",
        "            obs = self.normalize(obs)\n",
        "        else:\n",
        "            obs = self.normalize(np.array([obs]))[0]\n",
        "        if not return_info:\n",
        "            return obs\n",
        "        else:\n",
        "            return obs, info\n",
        "\n",
        "    def normalize(self, obs):\n",
        "        self.obs_rms.update(obs)\n",
        "        return (obs - self.obs_rms.mean) / np.sqrt(self.obs_rms.var + self.epsilon)\n",
        "\n",
        "\n",
        "class NormalizeReward(gym.core.Wrapper):\n",
        "    def __init__(\n",
        "        self,\n",
        "        env,\n",
        "        gamma=0.99,\n",
        "        epsilon=1e-8,\n",
        "    ):\n",
        "        super().__init__(env)\n",
        "        self.num_envs = getattr(env, \"num_envs\", 1)\n",
        "        self.is_vector_env = getattr(env, \"is_vector_env\", False)\n",
        "        self.return_rms = RunningMeanStd(shape=())\n",
        "        self.returns = np.zeros(self.num_envs)\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, rews, dones, infos = self.env.step(action)\n",
        "        if not self.is_vector_env:\n",
        "            rews = np.array([rews])\n",
        "        self.returns = self.returns * self.gamma + rews\n",
        "        rews = self.normalize(rews)\n",
        "        self.returns[dones] = 0.0\n",
        "        if not self.is_vector_env:\n",
        "            rews = rews[0]\n",
        "        return obs, rews, dones, infos\n",
        "\n",
        "    def normalize(self, rews):\n",
        "        self.return_rms.update(self.returns)\n",
        "        return rews / np.sqrt(self.return_rms.var + self.epsilon)"
      ],
      "metadata": {
        "id": "Qe1HXNCGDqIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym_ple.make(\"FlappyBird-v0\")"
      ],
      "metadata": {
        "id": "DiQoJy3vxQJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()\n",
        "env.unwrapped.game_state.getGameState()"
      ],
      "metadata": {
        "id": "3T5uqIaDxQMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(env.unwrapped.game_state.getGameState().values())"
      ],
      "metadata": {
        "id": "HjqgTgSByhBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.unwrapped.game_state.frame_skip = 4"
      ],
      "metadata": {
        "id": "YuR0dNuhyhDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StateVectorWrapper(gym.Wrapper):\n",
        "\n",
        "  def __init__(self, env):\n",
        "    super().__init__(env)\n",
        "    state = self.reset()\n",
        "    self.observation_space = gym.spaces.Box(\n",
        "        low=float('-inf'), \n",
        "        high=float('inf'),\n",
        "        shape=state.shape\n",
        "    )\n",
        "\n",
        "  def reset(self):\n",
        "    super().reset()\n",
        "    state_dict = self.env.unwrapped.game_state.getGameState()\n",
        "    state = list(state_dict.values())\n",
        "    return np.array(state)\n",
        "\n",
        "  def step(self, action):\n",
        "    _, reward, done, info = super().step(action)\n",
        "    next_state_dict = self.env.unwrapped.game_state.getGameState()\n",
        "    next_state = list(next_state_dict.values())\n",
        "    return np.array(next_state), reward, done, info"
      ],
      "metadata": {
        "id": "ULpe9kd9EzfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_environment(name):\n",
        "  env = gym_ple.make(name)\n",
        "  env = StateVectorWrapper(env)\n",
        "  env = NormalizeObservation(env)\n",
        "  env = NormalizeReward(env)\n",
        "  return env"
      ],
      "metadata": {
        "id": "NPM3zGRAxg3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2kQbrqV066I"
      },
      "outputs": [],
      "source": [
        "env = create_environment('FlappyBird-v0')\n",
        "frames = []\n",
        "\n",
        "for episode in range(10):\n",
        "  done = False\n",
        "  env.reset()\n",
        "  while not done:\n",
        "    frames.append(env.render(mode='rgb_array'))\n",
        "    action = env.action_space.sample()\n",
        "    _, _, done, _ = env.step(action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eL7ydsCHSA8T"
      },
      "outputs": [],
      "source": [
        "display_video(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgXi6A4Z1p75"
      },
      "source": [
        "#### Create the Deep Q-Learning algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOmxUJ1vnY5d"
      },
      "outputs": [],
      "source": [
        "class DeepQLearning(LightningModule):\n",
        "\n",
        "  # Initialize.\n",
        "  def __init__(self, env_name, policy=epsilon_greedy, capacity=100_000, \n",
        "               batch_size=256, lr=1e-3, hidden_size=128, gamma=0.99, \n",
        "               loss_fn=F.smooth_l1_loss, optim=AdamW, eps_start=1.0, eps_end=0.15, \n",
        "               eps_last_episode=100, samples_per_epoch=1_000, sync_rate=10):\n",
        "    \n",
        "    super().__init__()\n",
        "    self.env = create_environment(env_name)\n",
        "\n",
        "    obs_size = self.env.observation_space.shape[0]\n",
        "    n_actions = self.env.action_space.n\n",
        "\n",
        "    self.q_net = DQN(hidden_size, obs_size, n_actions)\n",
        "\n",
        "    self.target_q_net = copy.deepcopy(self.q_net)\n",
        "\n",
        "    self.policy = policy\n",
        "    self.buffer = ReplayBuffer(capacity=capacity)\n",
        "\n",
        "    self.save_hyperparameters()\n",
        "\n",
        "    while len(self.buffer) < self.hparams.samples_per_epoch:\n",
        "      print(f\"{len(self.buffer)} samples in experience buffer. Filling...\")\n",
        "      self.play_episode(epsilon=self.hparams.eps_start)\n",
        "    \n",
        "  @torch.no_grad()\n",
        "  def play_episode(self, policy=None, epsilon=0.):\n",
        "    state = self.env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "      if policy:\n",
        "        action = policy(state, self.env, self.q_net, epsilon=epsilon)\n",
        "      else:\n",
        "        action = self.env.action_space.sample()\n",
        "      next_state, reward, done, info = self.env.step(action)\n",
        "      exp = (state, action, reward, done, next_state)\n",
        "      self.buffer.append(exp)\n",
        "      state = next_state\n",
        "\n",
        "  # Forward.\n",
        "  def forward(self, x):\n",
        "    return self.q_net(x)\n",
        "\n",
        "  # Configure optimizers.\n",
        "  def configure_optimizers(self):\n",
        "    q_net_optimizer = self.hparams.optim(self.q_net.parameters(), lr=self.hparams.lr)\n",
        "    return [q_net_optimizer]\n",
        "\n",
        "  # Create dataloader.\n",
        "  def train_dataloader(self):\n",
        "    dataset = RLDataset(self.buffer, self.hparams.samples_per_epoch)\n",
        "    dataloader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=self.hparams.batch_size\n",
        "    )\n",
        "    return dataloader\n",
        "\n",
        "  # Training step.\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    states, actions, rewards, dones, next_states = batch\n",
        "    actions = actions.unsqueeze(1)\n",
        "    rewards = rewards.unsqueeze(1)\n",
        "    dones = dones.unsqueeze(1)\n",
        "\n",
        "    state_action_values = self.q_net(states).gather(1, actions)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      _, next_actions = self.q_net(next_states).max(dim=1, keepdim=True)\n",
        "      next_action_values = self.target_q_net(next_states).gather(1, next_actions)\n",
        "      next_action_values[dones] = 0.0\n",
        "\n",
        "    expected_state_action_values = rewards + self.hparams.gamma * next_action_values\n",
        "\n",
        "    loss = self.hparams.loss_fn(state_action_values, expected_state_action_values)\n",
        "    self.log('episode/Q-Error', loss)\n",
        "    return loss\n",
        "\n",
        "  # Training epoch end.\n",
        "  def training_epoch_end(self, training_step_outputs):\n",
        "\n",
        "    epsilon = max(\n",
        "        self.hparams.eps_end,\n",
        "        self.hparams.eps_start - self.current_epoch / self.hparams.eps_last_episode\n",
        "    )\n",
        "\n",
        "    self.play_episode(policy=self.policy, epsilon=epsilon)\n",
        "    self.log('episode/Return', self.env.unwrapped.game_state.score())\n",
        "\n",
        "    if self.current_epoch % self.hparams.sync_rate == 0:\n",
        "      self.target_q_net.load_state_dict(self.q_net.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mm9P0sX1wAA"
      },
      "source": [
        "#### Purge logs and run the visualization tool (Tensorboard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfGQdpn0nY99"
      },
      "outputs": [],
      "source": [
        "!rm -r /content/lightning_logs/\n",
        "!rm -r /content/videos/\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/lightning_logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8GdIwla1wrW"
      },
      "source": [
        "#### Train the policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ig8c_RM8nZLN"
      },
      "outputs": [],
      "source": [
        "algo = DeepQLearning(\n",
        "  'FlappyBird-v0',\n",
        "  lr=5e-4,\n",
        "  hidden_size=512,\n",
        "  eps_end=0.01,\n",
        "  eps_last_episode=1_000,\n",
        "  capacity=10_000,\n",
        "  gamma=0.9\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "  gpus=num_gpus,\n",
        "  max_epochs=3_000,\n",
        "  log_every_n_steps=1\n",
        ")\n",
        "\n",
        "trainer.fit(algo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jD3x39w71xWR"
      },
      "source": [
        "#### Check the resulting policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PleQkLR-yNM"
      },
      "outputs": [],
      "source": [
        "env = algo.env\n",
        "policy = algo.policy\n",
        "q_net = algo.q_net\n",
        "frames = []\n",
        "\n",
        "for episode in range(10):\n",
        "  done = False\n",
        "  obs = env.reset()\n",
        "  while not done:\n",
        "    frames.append(env.render(mode='rgb_array'))\n",
        "    action = policy(obs, env, q_net)\n",
        "    obs, _, done, _ = env.step(action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRwyLvCdCOO3"
      },
      "outputs": [],
      "source": [
        "display_video(frames)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jLwhTqDNiupZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "5_dueling_dqn.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}